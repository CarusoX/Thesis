\documentclass{beamer}
\mode<presentation>{
  \usetheme{metropolis}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\usepackage[spanish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{pgfpages}

\newif\ifnotes
\notesfalse

\newcommand{\notesmode}{\notestrue}

\ifnotes
  \setbeamertemplate{note page}[plain]
  \setbeameroption{show notes on second screen=right}
  % \setbeameroption{show notes}
\fi

\title[Procesamiento eficiente de gotas]{Desarrollo de un sistema eficiente para el procesamiento de datos experimentales de la
carga eléctrica de gotas de lluvia}
\author{Uziel Ludueña\\
Director: Rodolfo Guillermo Pereyra}
\institute{FaMAF -- UNC}
\date{19 de Diciembre de 2025}

\AtBeginSection[]{
  \begin{frame}{Agenda}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Agenda}
  \tableofcontents
\end{frame}

\section{Motivación y contexto}

\begin{frame}{Contexto}
  \begin{itemize}
    \item En el \textbf{grupo de física de la atmósfera de FaMAF} se estudian fenómenos meteorológicos y climáticos
    \vspace{0.5cm}
    \item Existe un \textbf{instrumento experimental} que mide la carga eléctrica de gotas de lluvia
    \vspace{0.5cm}
    \item El instrumento genera \textbf{grandes volúmenes de datos} que requieren procesamiento eficiente
  \end{itemize}
  
  \note{
    Comenzaré presentando el contexto de este trabajo.
    
    En el grupo de física de la atmósfera de FaMAF se realizan investigaciones sobre fenómenos meteorológicos y climáticos. Dentro de este grupo, ya existe un instrumento experimental desarrollado que es capaz de realizar mediciones simultáneas de la carga eléctrica de gotas de lluvia, así como de su tamaño y velocidad de caída.
    
    Este instrumento genera grandes volúmenes de datos que requieren un procesamiento eficiente para extraer la información relevante. Sin embargo, como veremos más adelante, el código existente para procesar estos datos presenta limitaciones tanto en diseño como en rendimiento.
  }
\end{frame}

\begin{frame}{¿Para qué medir la carga y el tamaño de las gotas?}
  \begin{itemize}
    \item Las gotas de lluvia adquieren su carga eléctrica a través de procesos microfísicos en la nube
    \vspace{0.5cm}
    \item El signo y magnitud de la carga dependen de condiciones termodinámicas de la tormenta
    \vspace{0.5cm}
    \item La distribución de tamaños de las gotas es clave para:
    \begin{itemize}
      \item Estimar precipitación
      \item Modelos numéricos y radar meteorológico
      \item Análisis de erosión y crecidas
    \end{itemize}
  \end{itemize}
  
  \note{
    Ahora explicaré por qué es importante estudiar las gotas de lluvia cargadas eléctricamente.
    
    Las gotas de lluvia no son simplemente agua que cae. Pueden transportar carga eléctrica que se adquiere dentro de la nube, principalmente a través de colisiones entre diferentes tipos de partículas como granizo, graupel, copos de nieve y cristales de hielo.
    
    El signo y la magnitud de esta carga dependen fuertemente de las condiciones termodinámicas y microfísicas dentro de la tormenta, lo cual está descrito por el mecanismo no-inductivo.
    
    Además, la distribución de tamaños de las gotas concentra información muy valiosa para estimar precipitación, alimentar modelos numéricos, interpretar datos de radar meteorológico y analizar procesos como erosión, escorrentía y crecidas.
    
    Por lo tanto, medir y analizar simultáneamente el tamaño y la carga de las gotas resulta fundamental para comprender la física de las tormentas y mejorar herramientas de pronóstico.
  }
\end{frame}

\begin{frame}{Instrumento y adquisición de datos}
  \begin{columns}
    \column{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{../escrito/figures/instrumento_de_medicion.png}
    
    \column{0.5\textwidth}
    \textbf{Instrumento existente:}
    \begin{itemize}
      \item Anillo de inducción + placa de aluminio
      \item Amplificadores de corriente ($5 \cdot 10^8$ V/A)
      \item Jaula de Faraday
    \end{itemize}
    
    \vspace{0.5cm}
    \textbf{Adquisición:}
    \begin{itemize}
      \item 5 kHz por canal
      \item Pérdida de 50ms/segundo
    \end{itemize}
  \end{columns}
  
  \note{
    El instrumento, descrito en el trabajo de Pereyra et al. 2025, consiste en un anillo de inducción de latón posicionado a 5.7 centímetros por encima de una placa plana de aluminio. Ambos están conectados a amplificadores de corriente de alta ganancia con una amplificación de 5 por 10 a la 8 voltios por amperio, y todo el sistema está protegido por una jaula de Faraday que protege contra interferencias electromagnéticas.
    
    La recolección de datos se realiza a 5 kilohertz por canal, es decir, 5000 datos por segundo. Sin embargo, debido a limitaciones de hardware, no se puede realizar la recolección al mismo tiempo que la escritura a disco, por lo que cada segundo se pierden aproximadamente 50 milisegundos de datos durante la escritura, lo cual produce huecos en la señal.
  }
\end{frame}

\begin{frame}{Teoría: ¿Cómo se ve una gota?}
  \begin{itemize}
    \item \textbf{Anillo:} Polaridad opuesta al acercarse, se invierte al alejarse
    \item \textbf{Placa:} Polaridad opuesta, meseta al impactar
  \end{itemize}
  \vspace{0.2cm}
  \begin{center}
  \includegraphics[width=0.65\textwidth]{../escrito/figures/corriente_gotas.png}
  \end{center}
  \vspace{0.1cm}
  Denominamos `pulso' a la señal que forma parte de una gota.
  
  \note{
    Ahora, en teoría, cuando una gota cargada eléctricamente cae, induce corrientes tanto en el anillo como en la placa.
    
    Primero, la gota se acerca al anillo. Al hacerlo, se induce una corriente de la polaridad opuesta a la carga de la gota. Luego, al alejarse del anillo, esta polaridad se invierte.
    
    Mientras tanto, la gota se acerca a la placa, induciendo también una corriente de polaridad opuesta a la carga de la gota. Cuando la gota impacta la placa de aluminio, se le transfiere toda la carga, lo cual produce una meseta en la corriente que se disipa en el tiempo.
    
    Esta es la señal ideal que esperaríamos ver. Sin embargo, la realidad es muy diferente.
  }
\end{frame}

\begin{frame}{La realidad: señal cruda}
  \begin{center}
  \includegraphics[width=0.7\textwidth,keepaspectratio]{./figures/tormenta_lvm.png}
  \end{center}
  
  \vspace{0.1cm}
  \textbf{El programa existente resuelve estos problemas:}
  \begin{itemize}
    \item Llena los huecos, mueve las señales de base a cero
    \item Identifica pulsos válidos entre el ruido continuo
    \item Extrae propiedades físicas de los pulsos
  \end{itemize}

  \note{
    Sin embargo, la realidad es muy diferente a la teoría. La señal que obtenemos del instrumento está llena de ruido electromagnético continuo que offsetea la señal constantemente.
    
    Este offset no es constante, sino que varía durante la medición, lo que hace que la señal base no esté en cero y cambie con el tiempo. Como pueden ver en la imagen, la señal está completamente contaminada por este ruido.
    
    A pesar de estos problemas, el programa existente ya resuelve estos desafíos: es capaz de detectar pulsos válidos entre todo este ruido continuo y extraer las propiedades físicas precisas de las gotas, incluso en ventanas muy cortas.
    
    Sin embargo, como veremos a continuación, el código actual presenta limitaciones tanto en diseño como en rendimiento que motivan la necesidad de mejoras.
  }
\end{frame}

\begin{frame}{Procesamiento de señales}
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/diagrama.png}
  
  \note{
    El procesamiento transforma las señales crudas registradas por el instrumento en una lista de propiedades físicas para cada gota. Para esto se emplean dos programas que se ejecutan de manera secuencial.
    
    El primer programa se encarga del preprocesamiento de los datos. Específicamente realiza dos operaciones fundamentales: primero, el rellenado de huecos mediante interpolación lineal para compensar los 50 milisegundos que se pierden cada segundo durante la adquisición. Segundo, la remoción del offset de las señales, eliminando el offset de corriente continua que varía durante la medición.
    
    El segundo programa realiza el análisis principal de las señales ya preprocesadas. Este programa implementa el algoritmo de detección de pulsos, aplica filtros de calidad para separar los pulsos válidos de los falsos positivos, y finalmente calcula las propiedades físicas de cada gota detectada, incluyendo carga eléctrica, velocidad de caída y tamaño.
  }
\end{frame}

\begin{frame}{Problemas y objetivos}
  \textbf{Problemas principales:}
  \begin{itemize}
    \item \textbf{Es lentísimo:} 6 horas para procesar una tormenta de 5 horas (100M datos)
    \item \textbf{Consume mucha memoria:} obliga a procesar los datos de a partes
    \item \textbf{Codigo monolítico:} Requiere entender todo el codigo para hacer modificaciones
  \end{itemize}

  \textbf{Objetivos:}
  \begin{itemize}
    \item Analizar el código existente para entender su funcionamiento
    \item Replicar su funcionalidad de forma más óptima
    \item Refactorizar el código para hacerlo más modular y mantenible
  \end{itemize}
  
  \note{
    El código actual, aunque funcional, presenta problemas serios de rendimiento. Es extremadamente lento: tarda 6 horas para procesar una tormenta de 5 horas, que son aproximadamente 100 millones de datos. Además, consume mucha memoria, lo que obliga a procesar los datos de a partes. El código es monolítico, lo que dificulta su mantenimiento y modificación.

    Para solucionar estos problemas, es necesario analizar el código existente para entender completamente su funcionamiento y poder replicar su funcionalidad de forma más óptima. El objetivo es mejorar significativamente el rendimiento y el uso de memoria, manteniendo la misma precisión en la detección de pulsos.
    Tambien es necesario refactorizar el código para hacerlo más modular y mantenible, lo cual facilita la implementación de nuevas funcionalidades y la corrección de errores.
  }
\end{frame}

\section{Conceptos previos}

\begin{frame}{Complejidad computacional y notación Big-O}
  Herramienta para evaluar y comparar la eficiencia de algoritmos
  
  \vspace{0.2cm}
  \textbf{Definición Big-O:}
  $$f(n) = O(g(n)) \text{ si existen constantes } c > 0 \text{ y } n_0 \text{ tales que}$$
  $$f(n) \leq c \cdot g(n) \text{ para todo } n \geq n_0$$
  
  \vspace{0.2cm}
  \centering
  \includegraphics[width=0.6\textwidth]{../escrito/figures/complejidades_algoritmicas.png}
  
  \note{
    Para evaluar y comparar la eficiencia de los algoritmos utilizados en este trabajo, es fundamental entender cómo se mide el rendimiento computacional. La notación Big-O es una herramienta matemática que permite describir el comportamiento asintótico de un algoritmo en términos de tiempo de ejecución o uso de memoria, en función del tamaño de la entrada.
    
    La notación Big-O describe el límite superior del crecimiento de una función. Formalmente, decimos que f(n) es igual a O(g(n)) si existen constantes positivas c y n_0 tales que f(n) es menor o igual a c multiplicado por g(n) para todo n mayor o igual a n_0.
    
    En el contexto de algoritmos, esto significa que el tiempo de ejecución o el uso de memoria no crecerá más rápido que la función g(n) multiplicada por una constante, independientemente del tamaño de la entrada.
    
    Las clases de complejidad más relevantes para este trabajo son:
    - O(1) constante: el tiempo es independiente del tamaño de la entrada, como acceder a una posición fija de un arreglo
    - O(log n) logarítmica: el tiempo crece logarítmicamente, como en búsqueda binaria
    - O(n) lineal: el tiempo es proporcional al tamaño de la entrada, como recorrer todos los elementos
    - O(n log n) lineal logarítmica: característico de algoritmos de divide y vencerás como merge sort
    - O(n²) cuadrática: el tiempo es proporcional al cuadrado de la entrada, como comparar cada par de elementos
    
    En este trabajo procesamos millones de puntos de datos. La diferencia entre algoritmos de complejidad O(n) y O(n²) puede resultar en diferencias enormes en el tiempo de procesamiento. Por ejemplo, para procesar 100 millones de puntos, un algoritmo O(n) podría completarse en minutos, mientras que un algoritmo O(n²) podría requerir días o semanas.
    
    En el gráfico pueden ver cómo crecen estas complejidades en escala logarítmica.
  }
\end{frame}

\begin{frame}{Importancia de la constante en la complejidad}

  En general, la funcion $g(n)$ es lo mas simple posible, es decir que si
  \begin{itemize}
    \item $f(n) = O(n)$
    \item $f(n) = O(n^2)$
    \item $f(n) = O(3n + 7)$
  \end{itemize}
  elegimos $O(n)$.

  \vspace{0.3cm}

  \textbf{La constante importa en la práctica:}
  \begin{itemize}
    \item Por definición: $f(n) = O(10n) = O(n)$ (la constante se elimina)
    \item Pero, un algoritmo que hace 10 veces más operaciones requiere 10 veces más cómputo
    \item Por lo tanto, es importante prestar atención a la cantidad de constante de un algoritmo, para terminar de evaluar su desempeño en la práctica.
  \end{itemize}
\end{frame}

\begin{frame}{Array (Arreglo)}
  \begin{center}
  \includegraphics[width=0.7\textwidth]{figures/array.png}
  \end{center}
  
  \vspace{0.3cm}
  \textbf{Características:}
  \begin{itemize}
    \item Lista de elementos en espacio contiguo de memoria
  \end{itemize}
  
  \vspace{0.2cm}
  \textbf{Operaciones:}
  \begin{itemize}
    \item \texttt{acceso[i]}: $O(1)$
  \end{itemize}
  
  \note{
    Un array, o arreglo, es un espacio contiguo de memoria que permite acceso directo a cualquier elemento mediante su índice. Esta estructura es fundamental porque permite acceso directo a cualquier elemento en tiempo constante O(1), simplemente calculando la posición en memoria basándose en el índice.
    
    Las operaciones principales son acceso y lectura por índice, ambas en tiempo constante O(1). Un ejemplo cotidiano es una lista de supermercado, donde puedes acceder directamente a cualquier elemento de la lista sabiendo su posición.
  }
\end{frame}

\begin{frame}{Queue (Cola)}
  \begin{center}
  \includegraphics[width=0.6\textwidth]{figures/queue.png}
  \end{center}
  
  \vspace{0.1cm}
  \textbf{Características:}
  \begin{itemize}
    \item FIFO: First In, First Out
  \end{itemize}
  
  \vspace{0.1cm}
  \textbf{Operaciones:}
  \begin{itemize}
    \item \texttt{push}: $O(1)$
    \item \texttt{pop}: $O(1)$
    \item \texttt{front}: $O(1)$
  \end{itemize}
  
  \vspace{0.1cm}
  \textbf{Ejemplo:} Cola del supermercado
  
  \note{
    Una cola, o queue, es una estructura de datos que sigue el principio FIFO - First In, First Out - donde los elementos se agregan por un extremo y se eliminan por el otro. Un ejemplo clásico es la cola del supermercado, donde la primera persona que llega es la primera en ser atendida.
    
    Una queue tiene tres operaciones básicas: push, que agrega un elemento al final de la queue, pop, que elimina el elemento del frente, y front, que lee el elemento del frente sin eliminarlo. Todas estas operaciones se pueden realizar en tiempo constante O(1).
  }
\end{frame}

\begin{frame}{Deque (Cola doble)}
  \begin{center}
  \includegraphics[width=0.6\textwidth]{figures/deque.png}
  \end{center}
  
  \vspace{0.1cm}
  \textbf{Características:}
  \begin{itemize}
    \item Inserción/eliminación por ambos extremos
  \end{itemize}
  
  \vspace{0.1cm}
  \textbf{Operaciones:}
  \begin{itemize}
    \item \texttt{push\_back}, \texttt{push\_front}: $O(1)$
    \item \texttt{pop\_back}, \texttt{pop\_front}: $O(1)$
    \item \texttt{front}, \texttt{back}: $O(1)$
  \end{itemize}
  
  \vspace{0.1cm}
  \textbf{Ejemplo:} Historial del navegador
  
  \note{
    Una cola doblemente terminada, o deque, es una estructura de datos que permite insertar y eliminar elementos por ambos extremos. Un ejemplo clásico es una escalera mecánica que pueda cambiar de dirección, permitiendo movimiento en ambas direcciones.
    
    La deque tiene operaciones de inserción y eliminación por ambos extremos: push_back y push_front para agregar elementos, pop_back y pop_front para eliminar elementos, y front y back para leer los elementos de los extremos sin eliminarlos. Todas estas operaciones se pueden realizar en tiempo constante O(1).
  }
\end{frame}

\section{Análisis del código existente}

\begin{frame}{Lectura de datos}
  \textbf{Formato de entrada:} archivo `.lvm' con 3 columnas
  
  \vspace{0.5cm}
  \centering
  \begin{tabular}{ccc}
    \toprule
    \textbf{Tiempo (s)} & \textbf{Señal Anillo} & \textbf{Señal Placa} \\
    \midrule
    0.000000 & 0.272755 & 0.162585 \\
    0.000200 & 0.272603 & 0.159991 \\
    0.000400 & 0.273518 & 0.145190 \\
    0.000600 & 0.278248 & 0.160296 \\
    0.000800 & 0.268635 & 0.154192 \\
    0.001000 & 0.276722 & 0.161211 \\
    0.001200 & 0.275044 & 0.150683 \\
    0.001400 & 0.272297 & 0.157855 \\
    0.001600 & 0.265736 & 0.154040 \\
    0.001800 & 0.277791 & 0.158770 \\
    \bottomrule
  \end{tabular}
  
  \note{
    El primer programa, promedio_general.f, recibe como entrada las señales crudas registradas por el instrumento. El formato de los archivos es un archivo .lvm que contiene 3 columnas: tiempo, señal del anillo y señal de la placa.
    
    Como pueden ver en la tabla, cada línea del archivo contiene un punto de tiempo junto con los valores de ambos sensores. El tiempo está en segundos y las señales están en voltios.
    
    El segundo programa, buscador_de_gotas.f, lee los archivos de datos preprocesados con el paso anterior. Se pueden leer hasta 36 archivos, y cada archivo puede contener hasta 13 millones de muestras.
  }
\end{frame}

\begin{frame}{Rellenado de huecos}
    \textbf{Rellenado de huecos:} Interpolación lineal entre puntos anterior y posterior
    \vspace{0.2cm}
    \begin{center}
    \includegraphics[width=0.7\textwidth]{./figures/interpolacion_puntual.png}
    \end{center}
\end{frame}

\begin{frame}{Remoción de offset}
    \textbf{Remoción de offset:} Para cada punto, se calcula el promedio de 5001 puntos circundantes y se resta ese valor
    \vspace{0.2cm}
    \begin{center}
    \includegraphics[width=0.7\textwidth]{./figures/remocion_offset.png}
    \end{center}
    \vspace{0.2cm}
    La implementación actual tiene complejidad $O(5001n) = O(n)$
\end{frame}

\begin{frame}{Detección de pulsos}
  \textbf{Busqueda por umbrales:}
  \begin{itemize}
    \item 1000 umbrales desde 2.00 hasta 0.02V
    \item Pares de puntos consecutivos que superen el umbral en cada señal, con separación máxima de 100 puntos entre señales
  \end{itemize}
  
  \begin{center}
  \includegraphics[width=0.6\textwidth]{../escrito/figures/sensores_gota.png}
  \end{center}
  En el peor caso, el algoritmo tiene complejidad $O(1000 \cdot 100 \cdot n) = O(n)$
  
  \note{
    El segundo programa implementa un sistema de umbrales que varía dinámicamente desde un valor superior de 2.00 hasta uno inferior de 0.02 a lo largo de 1000 pasos. Esta variación permite detectar pulsos de diferentes amplitudes.
    
    Para cada umbral, se buscan pares de puntos consecutivos que superen el umbral en ambas señales - anillo y placa - con una separación máxima de 100 puntos. Al primer punto del par consecutivo en cada señal, se lo denomina c1 y c2 respectivamente.
    
    En la figura pueden observar un pulso de ejemplo detectado con este algoritmo, junto con los máximos consecutivos de cada sensor. Además se observan distintos valores de umbrales, y se marca el umbral que encuentra el pulso.
  }
\end{frame}

\begin{frame}{Delimitación de pulsos}
  \centering
  \includegraphics[width=0.5\textwidth]{../escrito/figures/analisis_gota.png}
  
  \vspace{0.3cm}
  \textbf{Puntos relevantes:}
  \begin{itemize}
    \item \textbf{u1, u2:} Inicio de cada señal (retrocediendo desde c1, c2 hasta anulación)
    \item \textbf{p1:} Punto medio del anillo (avanzando desde c1 hasta anulación)
    \item \textbf{p2:} Punto de quiebre de la placa (análisis de la integral)
  \end{itemize}
  
  \vspace{0.2cm}
  Ancho máximo del pulso: 400 puntos
  
  \note{
    Una vez detectado un pulso, se determina el inicio de cada señal - u1 y u2 respectivamente - retrocediendo desde c1 y c2 hasta encontrar el punto donde la señal se anula.
    
    Se calcula también el punto medio de la señal del anillo - p1 - avanzando desde c1 hasta donde se anula la señal, y el punto de quiebre de la señal de la placa - p2 - mediante análisis de la integral de la misma.
    
    El algoritmo recorta cada pulso en base a la localización de estos puntos, considerando un ancho máximo de 400 puntos.
    
    En la figura pueden observar un pulso detectado junto con sus puntos relevantes marcados.
  }
\end{frame}

\begin{frame}{Cálculo de propiedades físicas}
  \begin{columns}
    \column{0.5\textwidth}
    \textbf{Carga eléctrica:}
    \begin{itemize}
      \item Integración de señales de ambos canales
      \item Factor de conversión por amplificación
    \end{itemize}
    
    \centering
    \includegraphics[width=0.9\textwidth]{../escrito/figures/calculo_carga_electrica.png}
    
    \column{0.5\textwidth}
    \textbf{Velocidad:}
    \begin{itemize}
      \item 5.7 cm entre anillo y placa
      \item Tiempo entre p1 y p2
    \end{itemize}
    
    \textbf{Diámetro:}
    \begin{itemize}
      \item Interpolación en tabla velocidad-diámetro
    \end{itemize}
    
    \centering
    \includegraphics[width=0.9\textwidth]{../escrito/figures/relacion_velocidad_diametro.png}
  \end{columns}
  
  \note{
    Para cada pulso detectado, se calculan las propiedades físicas.
    
    La carga eléctrica se calcula integrando la señal de ambos canales y aplicando el factor de conversión correspondiente a la amplificación del instrumento. La integración de la señal se fundamenta en el principio físico de que la carga eléctrica inducida en un conductor es proporcional a la integral temporal de la corriente que fluye a través de él. En la figura pueden observar el cálculo de carga eléctrica mediante integración de señales del anillo y placa.
    
    La velocidad de caída se determina mediante la separación conocida entre anillo y placa - 5.7 centímetros - dividida por el tiempo que hay entre el punto medio de la señal del anillo p1 y el punto de quiebre de la señal de la placa p2.
    
    El diámetro se obtiene interpolando en una tabla velocidad-diámetro precalculada. En la figura pueden observar la relación velocidad-diámetro para gotas de lluvia.
  }
\end{frame}

\begin{frame}{Filtros de calidad}

  Una vez detectados y delimitados los pulsos, se aplican los siguientes filtros de calidad:
  \begin{enumerate}
    \item \textbf{Carga:} $|q| > 0.2pC$ en ambos canales
    \item \textbf{Signo:} Mismo signo en ambos canales
    \item \textbf{Velocidad:} Entre 0.7125 y 11.4 m/s
  \end{enumerate}
  
  \vspace{0.3cm}
  \begin{center}
  \includegraphics[width=0.7\textwidth]{figures/drops_falla_velocidad.png}
  \end{center}
  
  \note{
    Durante el proceso de detección, el algoritmo identifica tanto pulsos válidos que corresponden a gotas de lluvia reales, así como falsos positivos claros que corresponden a ruido o detecciones espurias.
    
    Para resolver este problema, se implementan filtros de calidad para separar los pulsos válidos de los que no lo son. Estos filtros eliminan detecciones que no cumplen con propiedades físicas esperadas para gotas de lluvia reales. Los filtros se aplican de manera progresiva, donde cada etapa elimina un tipo específico de detección errónea.
    
    Primero, los filtros de carga: la carga de la gota tanto en el anillo como en la placa tiene que ser mayor a un valor mínimo de 0.2 en valor absoluto.
    
    Segundo, el filtro de signo: se verifica que la carga del pulso sea del mismo signo en ambos canales.
    
    Tercero, el filtro de velocidad: se verifica que la distancia temporal entre los picos del anillo y la placa corresponda a una velocidad de caída físicamente plausible, entre 0.7125 y 11.4 metros por segundo.
  }
\end{frame}

\begin{frame}{Sistema de evaluación y ordenamiento}
  \textbf{5 criterios de evaluación:}
  \begin{itemize}
    \item Diferencia con modelos teóricos (anillo y placa)
    \item Desviación de proporción carga (87\% anillo/placa)
    \item Desviación de proporción ancho (79\% placa/anillo)
    \item Proporción de señal en rango de ruido
  \end{itemize}
  
  \vspace{0.3cm}
  \begin{columns}
    \column{0.5\textwidth}
    \centering
    \textbf{Gota de buena calidad}
    \vspace{0.2cm}
    \includegraphics[width=0.9\textwidth]{figures/gota_buena.png}
    
    \column{0.5\textwidth}
    \centering
    \textbf{Gota de mala calidad}
    \vspace{0.2cm}
    \includegraphics[width=0.9\textwidth]{figures/gota_mala.png}

  \end{columns}
  
  \note{
    Una vez aplicados los filtros de calidad, las detecciones restantes corresponden principalmente a gotas. Algunas de estas presentan señales muy limpias y bien definidas, mientras que otras pueden tener pequeñas distorsiones, ruido residual, o características que sugieren mediciones menos precisas.
    
    Para clasificar las gotas obtenidas, se implementa un sistema de puntuación que evalúa la calidad de cada detección mediante cinco criterios independientes. Este sistema fue desarrollado mediante ajuste manual y experimental.
    
    Los criterios incluyen: diferencia con modelos teóricos para el anillo y la placa, desviación de la proporción esperada entre cargas del anillo y la placa - que experimentalmente es aproximadamente 87 por ciento -, desviación de la proporción esperada del ancho entre los picos - que es aproximadamente 79 por ciento -, y la proporción de señal en el rango de ruido.
    
    Las evaluaciones se combinan para obtener una puntuación total. Mientras más chico sea el valor de la puntuación, mejor es la calidad de la gota.
    
    En las imágenes pueden ver ejemplos de una gota de buena calidad y una de mala calidad, mostrando la diferencia en la limpieza y definición de las señales.
  }
\end{frame}

\begin{frame}{Conclusiones del análisis del programa}
  \begin{columns}[t]
    \column{0.4\textwidth}
    \vspace{0pt}
    \textbf{Problemas de diseño:}
    \begin{itemize}
      \item Sin modularización
      \item Variables sin contexto
      \item Código duplicado
      \item I/O hard-codeado
      \item Perdida de datos
      \item Filtros post-detección
    \end{itemize}
    
    \column{0.6\textwidth}
    \vspace{0pt}
    \textbf{Beneficios de una refactorización:}
    \begin{itemize}
      \item Mantenibilidad
      \item Reutilizabilidad
      \item Verificación
      \item Colaboración
    \end{itemize}
  \end{columns}
  
  \note{
    El código existente presenta problemas de diseño que dificultan su comprensión y mantenimiento.
    
    En primer lugar, no hay modularización. Todo el procesamiento está en el entorno global del programa, que maneja todos los pasos de procesamiento, como la detección de pulsos, cálculo de características, aplicación de filtros, escritura de resultados, etc. Cualquier modificación requiere entender por completo cómo interactúan las diferentes partes del programa.
    
    En segundo lugar, hay variables sin contexto semántico. El código utiliza variables como 's', 'ss', 'i', 'j', 'w', 'k', 'l', 'p', 'r', 'x', 'cont', 'contt', 'cc1', 'cc2', 'iii' sin ningún contexto. Es imposible determinar qué representa cada una sin leer todo el código. Además, están todas definidas en el principio del programa, lo cual hace que sea difícil entender su uso en el resto del código.
    
    En tercer lugar, requiere dividir el conjunto de datos en varias partes. El consumo de memoria es muy alto, por lo que el programa requiere dividir el conjunto de datos en varias partes y procesarlos secuencialmente. Esto hace que se pierdan algunos pulsos, ya que los que quedan en medio de los cortes no se procesan.
    
    En cuarto lugar, la lectura de archivos está hard-codeada. El programa requiere cambiar los nombres de los archivos a procesar, ya que los nombres de estos son constantes en el código en vez de ser parámetros. Además, hay un bloque de código que permite leer hasta 36 archivos, que se debe comentar o descomentar dependiendo de la cantidad en la que se dividió el conjunto de datos a procesar.
    
    Finalmente, hay mucha duplicación de código. La cantidad de código repetido es muy alta. Hay mucha lógica la cual solo tiene un cambio en el signo, un cambio en un if statement o un cambio en una variable. Esto hace que sea difícil mantener el código ya que hay que modificarlo en varios lugares.

    Todas las problemáticas mencionadas anteriormente - tanto de diseño como de rendimiento - motivan a realizar una refactorización del código. Esta refactorización tiene muchos beneficios.
    
    Primero, mantenibilidad. Cualquier corrección de errores o implementación de mejoras requiere revisar y entender menos código, ya que la lógica se encuentra en funciones más pequeñas.
    
    Segundo, reutilizabilidad. El código puede ser reutilizado para otros experimentos o instrumentos, en vez de tener que adaptar el código monolítico existente.
    
    Tercero, verificación. Permitiría realizar pruebas unitarias de cada función y no de todo el programa completo, lo que facilita la detección y corrección de errores.
    
    Cuarto, colaboración. Otros investigadores pueden entender y modificar el código sin invertir tanta cantidad de tiempo y esfuerzo estudiándolo.
    
    Por todas estas razones, es necesario resolver los problemas de diseño y rendimiento para mejorar significativamente el sistema.
  }
\end{frame}

\section{Estructuras de datos y algoritmos optimizados}

\begin{frame}{Algoritmos de ventana deslizante}
  \textbf{Concepto:}
  \begin{itemize}
    \item Mover una ventana de tamaño fijo a lo largo de un array
    \item Realizar operaciones sobre los elementos en los extremos de la ventana
    \item Al avanzar: eliminar valor más antiguo, agregar nuevo valor
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{Ventaja:}
  \begin{itemize}
    \item Actualiza el resultado incrementalmente
    \item Costo computacional independiente del tamaño de la ventana
    \item Solo depende de la operación realizada
  \end{itemize}
  
  \note{
    El algoritmo de ventana deslizante, o sliding window, es una técnica de optimización utilizada para resolver eficientemente problemas que involucran secuencias contiguas en arreglos.
    
    En términos generales, la técnica consiste en mover una ventana de tamaño fijo a lo largo de la secuencia de datos, realizando operaciones sobre los elementos contenidos en cada posición de la ventana. A medida que la ventana avanza, el valor más antiguo se elimina y el nuevo valor que ingresa se incorpora al cálculo, actualizando así el resultado con un costo computacional que depende solamente de la operación realizada, y no del tamaño de la ventana.
    
    Esta técnica es muy poderosa porque permite evitar recalcular todo desde cero en cada paso, lo que resulta en algoritmos mucho más eficientes.
  }
\end{frame}

\begin{frame}{Ejemplo: suma de una ventana deslizante}
  \centering
  \includegraphics[width=\textwidth]{figures/ventana_1.png}
\end{frame}

\begin{frame}{Ejemplo: suma de una ventana deslizante}
  \centering
  \includegraphics[width=\textwidth]{figures/ventana_2.png}
\end{frame}

\begin{frame}{Aplicación: promedio móvil}
  \textbf{El promedio es simplemente:}
  $$\text{promedio} = \frac{\text{suma de la ventana}}{\text{tamaño de la ventana}}$$
  
  \vspace{0.5cm}
  \textbf{Optimización:}
  \begin{itemize}
    \item Suma de la ventana: resuelta con ventana deslizante (2 operaciones)
    \item División: solo cuando necesitamos el promedio (1 operación)
    \item Total: 3 operaciones por punto, independiente del tamaño de ventana
  \end{itemize}
  
  \vspace{0.3cm}
  \centering
  \textcolor{red}{\textbf{Complejidad: $O(n)$ con constante muy baja}}
  
  \note{
    Una vez entendido el principio de la ventana deslizante, podemos aplicarlo al problema específico de cálculo del promedio móvil utilizado en este trabajo para remover el offset de la señal.
    
    El promedio es simplemente la suma de la ventana dividida por el tamaño de la ventana. Como ya sabemos cómo calcular la suma de la ventana eficientemente con la técnica de sliding window - usando solo dos operaciones - el promedio se calcula dividiendo esta suma por el tamaño de la ventana.
    
    De esta manera, la suma total de la ventana se mantiene actualizada de forma incremental, requiriendo solo dos operaciones por iteración independientemente del tamaño de la ventana. El promedio se calcula dividiendo esta suma por el tamaño de la ventana cuando lo necesitamos, lo que requiere una división adicional.
    
    Como resultado, tenemos un algoritmo de complejidad O(n) con una constante muy baja, ya que hacemos una sola división por punto, y dos operaciones para mantener la suma actualizada. Esto es mucho más eficiente que el algoritmo original que requería 5001 operaciones por punto.
  }
\end{frame}


\begin{frame}{MinQueue}
  \textbf{¿Qué resuelve?}
  \begin{itemize}
    \item Obtener el elemento mínimo de una queue en $O(1)$
    \item Sin MinQueue: buscar el mínimo en toda la queue cada vez = $O(n)$
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{Modelado como:}
  \begin{itemize}
    \item \textbf{Queue principal:} almacena todos los elementos (FIFO)
    \item \textbf{Deque auxiliar:} mantiene candidatos al mínimo en orden creciente
    \item El primer elemento del deque siempre es el mínimo actual
  \end{itemize}
  
  \vspace{0.3cm}
  \textbf{Operaciones:} \texttt{push}, \texttt{pop}, \texttt{min} - todas en $O(1)$ amortizado
  
  \note{
    La MinQueue es una estructura de datos que resuelve el problema de obtener el elemento mínimo de una queue en tiempo constante O(1), en vez de buscar el mínimo en toda la ventana cada vez, lo cual sería O(n).
    
    La MinQueue utiliza dos estructuras internamente: una queue principal para almacenar todos los elementos siguiendo el principio FIFO, y una deque auxiliar para mantener los candidatos al mínimo en orden creciente. La clave del algoritmo radica en mantener la deque en orden creciente, para que el primer elemento de esta siempre sea el mínimo actual.
    
    Las operaciones básicas son push, que agrega un elemento, pop, que elimina el elemento más antiguo, y min, que obtiene el elemento mínimo actual. Todas estas operaciones tienen complejidad O(1) amortizado.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 1}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_1.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_2.png}
  
  \note{
    Veamos un ejemplo paso a paso de cómo funciona la MinQueue. En estas primeras dos imágenes podemos ver las operaciones iniciales de push.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 2}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_3.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_4.png}
  
  \note{
    Continuamos con más operaciones de push. Noten cómo la deque mantiene los candidatos al mínimo en orden creciente.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 3}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_5.png}
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 4}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_6.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_7.png}

  \note{
    Observen cómo cuando se agregan elementos menores, estos eliminan elementos mayores de la deque, manteniendo siempre el orden creciente.
  }  
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 5}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_8.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_9.png}
  
  \note{
    Continuamos viendo cómo se mantiene la deque ordenada y cómo el mínimo siempre está en el frente.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 6}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_10.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_11.png}
  
  \note{
    Ahora vemos operaciones de pop. Cuando se elimina un elemento de la queue, si ese elemento es el mínimo actual (está en el frente de la deque), también se elimina de la deque.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 7}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_12.png}
  
  \note{
    Continuamos con más operaciones, mostrando cómo la estructura se mantiene consistente después de cada operación.
  }
\end{frame}

\begin{frame}{Ejemplo MinQueue - Parte 8}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/minqueue_13.png}
  \vspace{0.5cm}
  \includegraphics[width=0.8\textwidth]{figures/minqueue_14.png}
  
  \note{
    Finalizamos el ejemplo mostrando cómo la MinQueue mantiene correctamente el mínimo en todo momento, permitiendo acceder a él en tiempo constante.
  }
\end{frame}

\begin{frame}{Extensión a MaxQueue y MaxMinQueue}
  \textbf{MaxQueue:}
  \begin{itemize}
    \item Mantener deque en orden \textbf{decreciente}
    \item El primer elemento es el máximo
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{MaxMinQueue:}
  \begin{itemize}
    \item Dos deques auxiliares independientes:
    \begin{itemize}
      \item Uno para mínimo (orden creciente)
      \item Otro para máximo (orden decreciente)
    \end{itemize}
    \item Complejidad: $O(1)$ amortizado para todas las operaciones
    \item Espacio: doble de espacio auxiliar
  \end{itemize}
  
  \note{
    Es trivial adaptar la MinQueue para obtener una MaxQueue: simplemente cambiamos la condición de comparación en el bucle while de mayor que a menor que, manteniendo el deque auxiliar en orden decreciente en lugar de creciente. De esta manera, el primer elemento del deque será el máximo en vez del mínimo.
    
    Para mantener simultáneamente el mínimo y el máximo, utilizamos dos deques auxiliares independientes: uno para el mínimo en orden creciente y otro para el máximo en orden decreciente. Esta extensión mantiene la complejidad O(1) amortizado para todas las operaciones, aunque requiere el doble de espacio auxiliar.
    
    Esta estructura es muy útil cuando necesitamos tanto el mínimo como el máximo de una ventana deslizante, como en nuestro algoritmo de detección de pulsos.
  }
\end{frame}

\section{Mejoras aplicadas en el sistema}

\begin{frame}{Arquitectura del nuevo sistema}
  \textbf{Refactorización en C++:}
  \begin{itemize}
    \item La \textbf{stdlib} es bastante completa para implementar todas las estructuras y algoritmos necesarios
    \item Permite implementar eficientemente: ventanas deslizantes, MaxMinQueue, etc
  \end{itemize}
  
  \textbf{Automatización:}
  \begin{itemize}
    \item Script en \textbf{Python} para automatizar todo el pipeline de procesamiento
    \item Ejecución con un único comando
  \end{itemize}
  
  \textbf{Post-análisis:}
  \begin{itemize}
    \item Script en \textbf{Fortran} breve que muestra un ejemplo de post-análisis posible sobre los datos extraídos
  \end{itemize}
\end{frame}

\begin{frame}{Nuevo flujo de trabajo}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/nuevo_flujo_trabajo.png}
  \end{center}
\end{frame}

\begin{frame}{Mejora en el algoritmo de rellenado de huecos}
  En vez de interpolar entre los puntos adyacentes al hueco, se interpolan entre los promedios de los 2500 puntos anteriores y posteriores al hueco.
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/mejora_rellenado_huecos.png}
  \end{center}
\end{frame}

\begin{frame}{Optimización en el algoritmo de remoción de offset}
  \textbf{Algoritmo original:}
  \begin{itemize}
    \item Para cada punto: suma de 5001 valores y división
    \item Complejidad: $O(5001n) = O(n)$ con constante muy alta
  \end{itemize}
  
  \vspace{0.5cm}
  \textbf{Optimización aplicada:}
  \begin{itemize}
    \item Uso de \textbf{ventana deslizante}
    \item Reducción de \textbf{5001 operaciones} a solo \textbf{3 operaciones} por punto
    \item Misma complejidad: $O(n)$, pero con constante mucho menor
  \end{itemize}
  
  \vspace{0.3cm}
  \centering
  \textcolor{red}{\textbf{Reducción de $\sim 1600$ veces en operaciones por punto}}
\end{frame}

\begin{frame}{Cambios en la detección de pulsos}
  % papapapa
\end{frame}

\section{Resultados y cierre}

\begin{frame}{Resultados y benchmarking}
  \textbf{Configuración de compilación:}
  \begin{itemize}
    \item C++: \texttt{-std=c++17 -O3}
    \item Fortran: \texttt{-O2}
  \end{itemize}
  \vspace{0.5cm}
  \begin{itemize}
    \item Reducción de tiempo: 6 h \textrightarrow{} 5 min para 100 millones de datos.
    \item Uso de memoria estable por debajo de 2 GB.
    \item Detección de pulsos aumenta 1--5\% según tormenta.
  \end{itemize}
\end{frame}

\begin{frame}{Validación cualitativa}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../escrito/figures/calidad_detecciones.png}
  \end{center}
  \textbf{Resultados:}
  \begin{itemize}
    \item El algoritmo mejorado obtiene \textbf{mejor calidad} de gotas que el original
    \item Las gotas extras detectadas tienen en general un \textbf{buen puntaje de evaluación}
  \end{itemize}
  
  \note{
    Además de la mejora cuantitativa en la cantidad de gotas detectadas, se observó una mejora cualitativa en la calidad de las detecciones.
    
    Si observamos la proporción acumulada de la evaluación de las gotas obtenidas, vemos que el nuevo sistema obtiene, aunque por un margen pequeño, mejor calidad de gotas que el sistema original. Esto nos dice que las gotas extras detectadas tienen en general un buen puntaje de evaluación.
    
    La curva azul del algoritmo mejorado está consistentemente a la izquierda de la curva verde del algoritmo original, lo que significa que para cualquier proporción acumulada, el algoritmo mejorado tiene valores de evaluación más bajos, y recordemos que valores más bajos de evaluación significan mejor calidad.
  }
\end{frame}

\begin{frame}{Posibles mejoras}
  \begin{itemize}
    \item Mejora en la adquisición de datos: eliminar los huecos de 50ms cada segundo.
    \item Procesamiento en tiempo real: solo almacenar las gotas detectadas, sin apagar y encender el instrumento.
    \item Mejora en la detección de pulsos: explorar técnicas de machine learning como Support Vector Networks (SVN) o redes neuronales.
  \end{itemize}
\end{frame}

\begin{frame}
  \centering
  {\Large \textbf{Preguntas?}}
\end{frame}

\end{document}
