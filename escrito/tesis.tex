\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{caption}

\usepackage[backend=biber,style=numeric,sorting=nyt]{biblatex}

% Configuración para reducir el tamaño de los captions
\captionsetup{font=small}

% Configuración para reducir espacio en figuras
\setlength{\textfloatsep}{10pt}
\setlength{\floatsep}{10pt}
\setlength{\intextsep}{10pt}

% Configuración de hyperref
\hypersetup{
    pdftitle={Desarrollo de un sistema eficiente para el procesamiento de datos experimentales de la carga eléctrica de gotas de lluvia},
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=red
}

% Configuración de listings para código
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    backgroundcolor=\color{gray!10},
    extendedchars=true,
    inputencoding=utf8,
    literate={ñ}{{\~n}}1 {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1 {ü}{{\"u}}1,
    lineskip=-0.2pt,
    aboveskip=0pt,
    belowskip=0pt
}

\addbibresource{bibliografia.bib}

\pagestyle{fancy}
\pagenumbering{arabic}

% Información del documento
\title{\textbf{Desarrollo de un sistema eficiente para el procesamiento de datos experimentales de la carga eléctrica de gotas de lluvia}}
\author{Estudiante de Licenciatura en Ciencias de la Computación}
\date{\today}


\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

% Añadir título meta para navegadores con pdf.js (ejemplo: Chrome)
\usepackage{etoolbox}
\AtBeginDocument{%
  \hypersetup{pdfinfo={
    Title={Desarrollo de un sistema eficiente para el procesamiento de datos experimentales de la carga eléctrica de gotas de lluvia}
  }}%
}

\begin{document}
\onehalfspacing

\maketitle

\begin{abstract}
    El análisis de mediciones de gotas de lluvia cargadas eléctricamente en tormentas requiere procesar grandes volúmenes de datos, pero el código existente presenta limitaciones de diseño y rendimiento. Se desarrolló un sistema modular y optimizado que implementa algoritmos eficientes como ventanas deslizantes y estructuras de datos optimizadas. El sistema de automatización permite ejecutar toda la cadena de análisis con un único comando. Los resultados muestran una reducción del tiempo de procesamiento de 6 horas a 15 minutos para 100 millones de datos, manteniendo la misma precisión en la detección de gotas.

    \textbf{Palabras clave:} gotas cargadas eléctricamente, procesamiento de señales, optimización de algoritmos, C++, Python
\end{abstract}

\selectlanguage{english}
\begin{abstract}
    The analysis of measurements of electrically charged raindrops in storms requires processing large data volumes, but existing code has design and performance limitations. A modular and optimized system was developed implementing efficient algorithms like sliding windows and optimized data structures. The automation system allows executing the entire analysis chain with a single command. Results show processing time reduction from 6 hours to 15 minutes for 100 million data points, maintaining the same droplet detection accuracy.

    \textbf{Keywords:} electrically charged droplets, signal processing, algorithm optimization, C++, Python
\end{abstract}
\selectlanguage{spanish}

\tableofcontents
\newpage

\section{Introducción}
\lhead{}
\rhead{Introducción}

Las gotas de lluvia pueden transportar carga eléctrica, la cual se adquiere principalmente a través de procesos microfísicos dentro de la nube, como las colisiones entre distintos hidrometeoros (granizo, graupel, copos de nieve, cristales de hielo). El signo y la magnitud de esa carga dependen fuertemente de las condiciones termodinámicas y microfísicas internas de la tormenta, tal como describe el mecanismo no-inductivo (NIM) \cite{avila2022}. Al mismo tiempo, la distribución de tamaños de las gotas (RSD) concentra información clave para estimar precipitación, alimentar modelos numéricos, interpretar teledetección/radar meteorológico y analizar procesos como erosión, escorrentía y crecidas \cite{martinez2023}.

En este sentido, medir y analizar simultáneamente el tamaño y la carga de las gotas resulta fundamental, ya que nos permite recopilar información que no solo ayuda a comprender la física de las tormentas, sino que también es útil para mejorar herramientas de pronóstico y para el estudio de fenómenos atmosféricos.

En este marco, en el grupo de física de la atmósfera de FaMAF, ya existe un instrumento capaz de realizar estas mediciones, como el descrito en `New experimental device for measuring electrical charge of precipitation particles' \cite{pereyra2025}, así como un programa para procesarlas. Sin embargo, el código disponible actualmente presenta limitaciones tanto en diseño como en rendimiento: está poco modularizado, resulta difícil de mantener y su desempeño es insuficiente para procesar de manera eficiente grandes volúmenes de datos. En esta tesis se propone un nuevo sistema de análisis que resuelve esas limitaciones, mejorando la eficiencia, la escalabilidad y la mantenibilidad del software, y facilitando así la obtención de resultados más completos y confiables.

\subsection{Instrumento de Medición}
\lhead{}
\rhead{Instrumento de Medición}

En la figura \ref{fig:instrumento_medicion} se muestra el dispositivo, el cual consiste en un anillo de inducción de latón de 6 cm de diámetro y 1.5 cm de longitud, posicionado a 5.7 cm por encima de una placa plana de aluminio de 20 cm de diámetro. Tanto el anillo como la placa están conectados eléctricamente a amplificadores de corriente de alta ganancia con una amplificación de 5 $\cdot$ 10$^8$ V/A. Para proteger contra interferencias electromagnéticas, el anillo, la placa y los amplificadores están encerrados dentro de un contenedor metálico que actúa como jaula de Faraday. A su vez, los amplificadores están protegidos contra daños por agua al estar asegurados dentro de un recinto impermeable.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/instrumento_de_medicion.png}
    \caption{(a) Diagrama del dispositivo de medición; (b) circuito amplificador inversor de corriente; (c) fotografía del dispositivo en el techo de la Facultad de Matemática, Astronomía, Física y Computación, Universidad Nacional de Córdoba.}
    \label{fig:instrumento_medicion}
\end{figure}

La jaula de Faraday presenta una abertura ligeramente mayor que el anillo, permitiendo que las gotas de lluvia entren únicamente a través del anillo de inducción. Toda el agua que llega a la placa se drena a través de sus lados y se recolecta en un contenedor de aluminio, que también está situado dentro de la jaula de Faraday pero eléctricamente aislado de ella.

Las gotas cargadas eléctricamente inducen corrientes tanto en el anillo como en la placa. Al caer una gota, primero se acerca al anillo. Al hacerlo, se induce una corriente de la polaridad opuesta a la carga de la gota. Luego, al alejarse, esta polaridad se invierte. Mientras tanto, la gota se acerca a la placa, induciendo también una corriente de polaridad opuesta a la carga de la gota, lo cual culmina en una meseta en la corriente dado que la placa de aluminio absorbe el impacto de la gota y se le transfiere toda la carga a la placa, la cual se disipa en el tiempo. La figura \ref{fig:corriente_gotas} muestra la dinámica de la señal eléctrica registrada por los amplificadores.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/corriente_gotas.png}
    \caption{Corriente inducida por una gota cargada negativamente en el anillo (puntos sólidos) y la placa (puntos huecos).}
    \label{fig:corriente_gotas}
\end{figure}

La recolección de datos se realiza a una tasa de 5 kHz por canal (5000 datos por segundo). Por limitaciones de hardware, no se puede realizar la recolección al mismo tiempo que la escritura a disco, por lo que cada segundo se pierden aproximadamente $50ms$ de datos, lo cual produce un "hueco" de datos en la señal.

\subsection{Procesamiento de Señales}
\lhead{}
\rhead{Procesamiento de Señales}

El procesamiento transforma las señales crudas registradas por el instrumento en una lista de propiedades físicas para cada gota. Para ello se emplean dos programas que se ejecutan de manera secuencial: el primero realiza un preprocesamiento de las señales y el segundo extrae la información de las gotas y calcula sus características.

El fragmento de la señal que forma parte de una gota extraída por el programa, la denominamos `pulso'. Estos pulsos son reconocibles en la señal gracias a la dinámica producida por la interacción de la gota con el instrumento de medición.

El flujo general consta de cinco pasos principales:

\begin{enumerate}
    \item Rellenado de huecos en los datos.
    \item Remoción del offset de las señales.
    \item Búsqueda de pulsos en las señales.
    \item Aplicación de filtros de calidad para extraer los pulsos.
    \item Cálculo de propiedades de los pulsos.
\end{enumerate}

Para el procesamiento de los datos se han desarrollado dos programas en Fortran que trabajan de manera secuencial.

El primer programa se encarga del preprocesamiento de los datos, específicamente del rellenado de huecos mediante interpolación lineal y de la remoción del offset de las señales.

El segundo programa realiza el análisis principal de las señales ya preprocesadas. Implementa el algoritmo de detección de pulsos, aplica filtros para separar los pulsos válidos y calcula sus propiedades físicas, incluyendo carga eléctrica, velocidad de caída y tamaño.

\subsection{Necesidad de Mejoras}
\lhead{}
\rhead{Necesidad de Mejoras}

El código actual, aunque funcional, presenta varios problemas tanto en su diseño como en su rendimiento.

En primer lugar, el código consta de pocos archivos con una gran cantidad de líneas, lo que dificulta su mantenimiento y modificación. Cualquier cambio requiere revisar y entender todo el código, ya que las responsabilidades no están bien asignadas. Además, hay muchas variables sin nombres descriptivos y constantes hard-codeadas sin referencia alguna, lo que complica aún más la comprensión.

En segundo lugar, para grandes volúmenes de datos, la ejecución es muy lenta. Por ejemplo, para procesar datos de una tormenta de 5 horas de duración (aproximadamente 100 millones de datos), el programa tarda alrededor de 6 horas en completarse. A esto se suma un consumo de memoria excesivamente alto, lo que obliga a procesar los datos de a partes. A su vez el algoritmo actual no tiene en cuenta los pulsos que quedan en medio de estos cortes, resultando en la pérdida de algunos datos.

\subsection{Objetivos}
\lhead{}
\rhead{Objetivos}

\subsubsection{Objetivo General}

Desarrollar un sistema modular y automatizado para el análisis de datos de gotas cargadas eléctricamente que mejore significativamente la eficiencia, mantenibilidad y escalabilidad del código existente, facilitando la obtención de nuevos resultados.

El nuevo sistema debe ser capaz de procesar grandes volúmenes de datos de manera eficiente y, como mínimo, detectar la misma cantidad de pulsos que el código original, intentando incrementarla en lo posible.

\subsubsection{Objetivos Específicos}

\begin{enumerate}
    \item \textbf{Diseñar una arquitectura modular} que separe las etapas del procesamiento en al menos tres componentes independientes y reutilizables.

    \item \textbf{Optimizar el rendimiento y el uso de memoria} del algoritmo de detección de pulsos para manejar aproximadamente 100 millones de muestras (que corresponden a una tormenta de 5 horas de duración) utilizando menos de 2 GB de memoria y reduciendo el tiempo de análisis de 6 horas a aproximadamente 15 minutos, asegurando al menos la misma cantidad de pulsos detectados que el programa original.

    \item \textbf{Desarrollar un sistema de automatización} que permita ejecutar la cadena completa de análisis con un único comando y sin intervención manual.

    \item \textbf{Documentar completamente} el sistema mediante un manual de usuario y comentarios en el código para facilitar su uso y mantenimiento futuro.
\end{enumerate}

\subsection{Estructura de la Tesis}
\lhead{}
\rhead{Estructura de la Tesis}

La estructura de esta tesis está organizada de manera que primero se establece el contexto y la motivación del trabajo, luego se presenta el análisis del sistema existente, y finalmente se describe la solución propuesta y sus resultados. El documento se divide en las siguientes secciones:

\begin{enumerate}
    \item Presenta los conceptos previos necesarios, incluyendo una introducción al análisis de complejidad computacional que fundamenta las optimizaciones posteriores. (Sección 2)

    \item Realiza un análisis exhaustivo del código existente, describiendo los algoritmos actuales, identificando problemas de diseño y rendimiento, y motivando la necesidad de refactorización. (Sección 3)

    \item Presenta los algoritmos optimizados y estructuras de datos eficientes utilizados en el nuevo sistema. (Sección 4)

    \item Describe las mejoras implementadas en el preprocesamiento de señales. (Sección 5)

    \item Describe el nuevo algoritmo de detección de pulsos implementado. (Sección 6)

    \item Presenta el sistema de automatización desarrollado en Python. (Sección 7)

    \item Muestra los resultados obtenidos, incluyendo comparativas de rendimiento y precisión, seguida de las conclusiones y limitaciones del trabajo. (Sección 8)
\end{enumerate}

\section{Conceptos previos}

\subsection{Análisis de complejidad computacional}
\lhead{}
\rhead{Análisis de Complejidad Computacional}

Para evaluar y comparar la eficiencia de los algoritmos utilizados en este trabajo, es fundamental entender cómo se mide el rendimiento computacional. La notación Big-O es una herramienta matemática que permite describir el comportamiento asintótico de un algoritmo en términos de tiempo de ejecución o uso de memoria, en función del tamaño de la entrada. \cite{cormen2009introduction}

\subsubsection{Notación Big-O}

La notación Big-O describe el límite superior del crecimiento de una función. Formalmente, decimos que $f(n) = O(g(n))$ si existen constantes positivas $c$ y $n_0$ tales que $f(n) \leq c \cdot g(n)$ para todo $n \geq n_0$.

En el contexto de algoritmos, esto significa que el tiempo de ejecución o el uso de memoria no crecerá más rápido que la función $g(n)$ multiplicada por una constante, independientemente del tamaño de la entrada.

\subsubsection{Clases de complejidad comunes}

Las clases de complejidad más relevantes para este trabajo son:

\begin{itemize}
    \item \textbf{$O(1)$ - Constante:} El tiempo de ejecución es independiente del tamaño de la entrada. Ejemplo: acceder a un elemento de un arreglo por índice.
    
    \item \textbf{$O(\log n)$ - Logarítmica:} El tiempo crece logarítmicamente con el tamaño de la entrada. Ejemplo: búsqueda binaria en un arreglo ordenado. La búsqueda binaria divide repetidamente el espacio de búsqueda por la mitad, eliminando la mitad de los elementos en cada iteración.
    
    \item \textbf{$O(n)$ - Lineal:} El tiempo es proporcional al tamaño de la entrada. Ejemplo: recorrer todos los elementos de un arreglo.
    
    \item \textbf{$O(n \log n)$ - Lineal logarítmica:} El tiempo crece como $n$ multiplicado por $\log n$. Ejemplo: algoritmos de ordenamiento eficientes como merge sort. Merge sort divide recursivamente el arreglo en mitades, ordena cada mitad y luego combina los resultados ordenados.
    
    \item \textbf{$O(n^2)$ - Cuadrática:} El tiempo es proporcional al cuadrado del tamaño de la entrada. Ejemplo: algoritmos de ordenamiento simples como bubble sort. Bubble sort compara pares adyacentes de elementos y los intercambia si están en orden incorrecto, repitiendo este proceso hasta que no se requieran más intercambios.
\end{itemize}

En la figura \ref{fig:complejidades_algoritmicas} se puede observar un gráfico comparativo de número de operaciones versus tamaño de datos para diferentes complejidades algorítmicas

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/complejidades_algoritmicas.png}
    \caption{Gráfico comparativo de número de operaciones versus tamaño de datos para diferentes complejidades algorítmicas, en escala logarítmica. }
    \label{fig:complejidades_algoritmicas}
\end{figure}

\subsubsection{Importancia de la constante en la complejidad}

Aunque por definición la notación nos dice que $O(10n) = O(n)$ (la constante se elimina), es importante entender que un algoritmo que haga 10 veces más operaciones que otro, requerirá obviamente 10 veces más cómputo. Por esta razón, es importante no solo prestar atención a la complejidad de los algoritmos, sino también a la `constante' que afecta a esta, ya que este factor puede ser determinante en la práctica.

\subsubsection{Importancia en el contexto de este trabajo}

En el procesamiento de señales, donde se manejan millones de puntos de datos, la diferencia entre algoritmos de complejidad $O(n)$ y $O(n^2)$ puede resultar en diferencias de horas en el tiempo de procesamiento. Por ejemplo, para procesar 100 millones de puntos (teniendo en cuenta que un procesador realizar aproximadamente $10^8$ operaciones por segundo):

\begin{itemize}
    \item Un algoritmo $O(n)$ podría completarse en segundos
    \item Un algoritmo $O(n^2)$ podría requerir días o semanas
\end{itemize}

Por esta razón, la optimización de la complejidad computacional, así como la optimización de la constante, es crucial para hacer viable el análisis de grandes volúmenes de datos meteorológicos en tiempo razonable.

\section{Análisis del código existente}
\lhead{}
\rhead{Análisis del código existente}

Antes de comenzar a desarrollar el nuevo sistema, fue fundamental analizar el código existente. El programa consta de dos archivos Fortran (`promedio\_general.f' y `buscador\_de\_gotas.f') donde toda la lógica está implementada directamente en el entorno global del programa, sin modularización alguna. 

\subsection{Descripción del algoritmo actual}
A continuación se describe el funcionamiento de los dos programas que conforman el sistema actual.

\subsubsection{Programa de preprocesamiento}

El primer programa (`promedio\_general.f') recibe como entrada las señales crudas registradas por el instrumento y realiza dos operaciones fundamentales:    

\begin{enumerate}


\item \textbf{Lectura de datos:} El programa lee los archivos de datos crudos y los convierte en señales. El formato de los archivos es un archivo `.lvm' que contiene 3 columnas: tiempo, señal del anillo y señal de la placa.

\item \textbf{Rellenado de huecos mediante interpolación:} Debido a las limitaciones de
hardware mencionadas anteriormente, cada segundo se pierden aproximadamente
$50ms$ de datos durante la adquisición. El programa identifica estos huecos y los
rellena utilizando interpolación lineal el punto anterior y el posterior al hueco.

\item \textbf{Remoción del offset de las señales:} Para cada punto de la señal, se calcula el
promedio de los 5000 puntos circundantes (2500 puntos a cada lado) y se le resta este
valor. Para esto se ignoran los primeros y últimos 2500 puntos de la señal.

Este proceso elimina el offset de corriente continua que puede variar durante la medición.

\item \textbf{Salida de resultados:} Finalmente, se escriben las señales preprocesadas en un archivo. El formato de salida son 4 columnas: contador de tiempo (entero), señal del anillo resultante, señal de la placa resultante y un booleano que indica si la fila corresponde a un hueco.

\end{enumerate}

\subsubsection{Programa de detección de pulsos}

El segundo programa (`buscador\_de\_gotas.f') implementa el algoritmo principal de
detección y análisis. El flujo general consta de las siguientes etapas:

\begin{enumerate}

\item \textbf{Lectura y preprocesamiento de datos:} El programa lee los archivos de datos
preprocesados con el paso anterior. Se pueden leer hasta 36 archivos, y cada archivo puede contener hasta 13 millones de muestras.

\item \textbf{Detección de pulsos mediante umbrales:} Se implementa un sistema de umbrales que varía dinámicamente desde un valor superior (`2.00') hasta
uno inferior (`0.02') a lo largo de 1000 pasos. Esta variación permite
detectar pulsos de diferentes amplitudes.
Para cada umbral, se buscan pares de puntos consecutivos que superen el umbral en ambas señales (anillo y placa), con una separación máxima de 100 puntos. Al primer punto del par consecutivo en cada señal, se lo denomina `c1' y `c2' respectivamente. En la figura \ref{fig:sensores_gota} se puede observar un pulso de ejemplo detectado con este algoritmo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sensores_gota.png}
    \caption{En la figura se puede observar un pulso, junto con los máximos consecutivos de cada sensor. Además se observan distintos valores de umbrales, y se marca el umbral que encuentra el pulso. }
    \label{fig:sensores_gota}
\end{figure}

\item \textbf{Localización y delimitación de pulsos:} Una vez detectado un pulso, se determina el "inicio" de cada señal (u1 y u2 respectivamente), retrocediendo desde c1 y c2 respectivamente hasta encontrar el punto donde la señal se anula.
Se calcula también el punto medio de la señal del anillo (p1) avanzando desde c1 hasta donde se anula la señal, y el punto de quiebre de la señal de la placa (p2) mediante análisis de la integral de la misma. En la figura \ref{fig:analisis_gota} se pueden observar estos puntos relevantes.
El algoritmo recorta cada pulso en base a la localización de estos puntos, considerando un ancho máximo de 400 puntos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/analisis_gota.png}
    \caption{La figura muestra un pulso detectado junto con sus puntos relevantes.}
    \label{fig:analisis_gota}
\end{figure}


\item \textbf{Cálculo de propiedades físicas:} Para cada pulso detectado, se calculan:

\begin{itemize}
    \item Carga eléctrica: Se integra la señal de ambos canales y se aplica el factor de
    conversión correspondiente a la amplificación del instrumento. La integración de la señal se fundamenta en el principio físico de que la carga eléctrica inducida en un conductor es proporcional a la integral temporal de la corriente que fluye a través de él. En la figura \ref{fig:calculo_carga_electrica} se puede observar el cálculo de carga eléctrica mediante integración de señales del anillo y placa.
    \item Velocidad de caída: Se determina mediante la separación conocida entre
    anillo y placa (5.7 cm) dividida por el tiempo que hay entre el punto medio de la señal del anillo y el punto de quiebre de la señal de la placa.
    \item Diámetro: Se obtiene interpolando en una tabla velocidad-diámetro precalculada. En la figura \ref{fig:relacion_velocidad_diametro} se puede observar la relación velocidad-diámetro para gotas de lluvia.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/calculo_carga_electrica.png}
    \caption{Cálculo de carga eléctrica mediante integración de señales del anillo y placa. Se debe tener en cuenta que la integral es de signo opuesto ya que la señal es inversora.}
    \label{fig:calculo_carga_electrica}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/relacion_velocidad_diametro.png}
    \caption{Relación velocidad-diámetro para gotas de lluvia.}
    \label{fig:relacion_velocidad_diametro}
\end{figure}


\item \textbf{Aplicación de filtros de calidad:} Durante el proceso de detección, el algoritmo identifica tanto pulsos válidos (que corresponden a gotas de lluvia reales), así como falsos positivos claros (que corresponden a ruido o detecciones espurias)

Para resolver este problema, se implementan filtros de calidad para separar los pulsos válidos de los que no lo son. Estos filtros eliminan detecciones que no cumplen con propiedades físicas esperadas para gotas de lluvia reales. Los filtros se aplican de manera progresiva, donde cada etapa elimina un tipo específico de detección errónea:

\begin{itemize}

\item \textbf{Filtros de carga:} La carga de la gota tanto en el anillo como en la placa tiene
que ser mayor a un valor mínimo (`qminima=0.2') (en valor absoluto).

\item \textbf{Filtro de signo:} Se verifica que la carga del pulso sea del mismo signo en
ambos canales.

\item \textbf{Filtro de velocidad:} Se verifica que la distancia temporal entre los picos del
anillo y la placa corresponda a una velocidad de caída físicamente plausible
(entre 0.7125 y 11.4 m/s).

\end{itemize}

\item \textbf{Sistema de evaluación y ordenamiento:} Una vez aplicados los filtros de calidad, las detecciones restantes corresponden principalmente a gotas. Algunas de estas presentan señales muy limpias y bien definidas, mientras que otras pueden tener pequeñas distorsiones, ruido residual, o características que sugieren mediciones menos precisas.

Para clasificar las gotas obtenidas, se implementa un sistema de puntuación que evalúa la calidad de cada detección mediante cinco criterios independientes. Este sistema fue desarrollado mediante ajuste manual y experimental, donde se analizaron visualmente detecciones para identificar patrones que distinguían entre gotas de alta y baja calidad.

\begin{itemize}

    \item \textbf{Eval1 y Eval2:} Diferencia cuadrática entre la integral de la señal real y dos modelos teóricos distintos
    del pulso.
    Para el pulso del anillo se utiliza el siguiente modelo:
    
    \begin{equation}
        a_1(t) = \frac{30.6 \cdot q_1}{\left(30.6^{1/1.7} + \left(\frac{v}{50.7}\right)^2(t-p_1)^2\right)^{1.7}}
    \end{equation}
    
    Para el pulso de la placa se utiliza un modelo por partes:

    \begin{equation}
        a_2(t) = \begin{cases}
            0 & \text{si } t < u_2 \\
            q_2 \cdot \left(\frac{t-u_2}{p_2-u_2}\right)^{1.373} & \text{si } u_2 \leq t < p_2 \\
            q_2 & \text{si } t \geq p_2
        \end{cases}
    \end{equation}

    donde $q_1$ y $q_2$ son las cargas medidas en el anillo y la placa respectivamente, $v$ es la velocidad de la gota, $p_1$ es el punto medio del pulso del anillo, $p_2$ es el punto de quiebre de la integral del pulso de la placa, y $u_2$ es el punto inicial del pulso de la placa.

    En la figura \ref{fig:comparacion_modelos_teoricos} se puede observar la comparación entre señales reales y modelos teóricos para pulsos del anillo y placa.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{figures/comparacion_modelos_teoricos.png}
        \caption{Comparación entre señales reales (integrales) y modelos teóricos para pulsos del anillo y placa.}
        \label{fig:comparacion_modelos_teoricos}
    \end{figure}

    \item \textbf{Eval3:} Desviación de la proporción esperada entre cargas del anillo y la placa. Experimentalmente se observó que la carga medida en el anillo es aproximadamente el 87\% de la carga medida en la placa, por lo que se mide la desviación de esta proporción.

    \begin{equation}
        penal2(l) = \left|\frac{q_1}{q} - 0.87 \cdot \frac{q_2}{q}\right|
    \end{equation}

    donde $q$ está definido como $q = \frac{q1 / 0.87 + q2}{2}$
    
    \item \textbf{Eval4:} Desviación de la proporción esperada del ancho entre los picos de la
señal de la placa y el anillo. Experimentalmente se observó que el ancho del pulso de la placa es aproximadamente el 79\% del ancho del pulso del anillo, por lo que se mide la desviación de esta proporción.

    \begin{equation}
        penal3(l) = \left|\frac{p_1}{p} - 0.79 * \frac{p_2}{p}\right|
    \end{equation}

    donde $p$ está definido como $p = \frac{p_1 / 0.79 + p_2}{2}$

    \item \textbf{Eval5:} Proporción de señal en el rango de ruido (entre -0.02 y 0.02). Para esto se computan los promedios de cada señal, y se cuentan los puntos que, cuando se les resta el promedio, están entre -0.02 y 0.02.

\end{itemize}

    Las evaluaciones se combinan linealmente con pesos para finalmente obtener una puntuación total, que está en el rango $[0, 20]$. Mientras más chico sea el valor de la puntuación total, mejor es la calidad de la gota.
    
\item \textbf{Salida de resultados:} Finalmente, se escriben los datos de todos los pulsos válidos
en un archivo, incluyendo posición temporal, cargas, velocidad, diámetro y puntuación de calidad.

\end{enumerate}

\subsection{Problemas de diseño}

El código existente presenta problemas de diseño que dificultan su comprensión y mantenimiento:

\begin{itemize}

\item \textbf{Modularización inexistente:} Todo el procesamiento está en el entorno global del programa, que maneja todos los pasos de procesamiento, como la detección de pulsos, cálculo de características, aplicación de filtros, escritura de resultados, etc. Cualquier modificación requiere entender por completo cómo interactúan las diferentes `partes' del programa.

\item \textbf{Variables sin contexto semántico:} El código utiliza variables como `s', `ss', `i', `j', `w', `k', `l', `p', `r', `x', `cont', `contt', `cc1', `cc2', `iii' sin ningún contexto. Es
imposible determinar qué representa cada una sin leer todo el código. Además, están todas definidas en el principio del programa, lo cual hace que sea difícil entender su uso en el resto del código.
    
\item \textbf{Requiere dividir el conjunto de datos en varias partes}: El consumo de memoria es muy alto, por lo que el programa requiere dividir el conjunto de datos en varias partes y procesarlos secuencialmente. Esto hace que se pierdan algunos pulsos, ya que los que quedan en medio de los cortes no se procesan.

\item \textbf{Lectura de archivos hard-codeada:} El programa requiere cambiar los nombres de los archivos a procesar, ya que los nombres de estos son constantes en el código en vez de ser parámetros. Además, hay un bloque de código que permite leer hasta 36 archivos, que se debe comentar o descomentar dependiendo de la cantidad en la que se dividió el conjunto de datos a procesar.

\item \textbf{Código duplicado:} La cantidad de código repetido es muy alta. Hay mucha lógica la cual solo tiene un cambio en el signo, un cambio en un `if statement' o un cambio en una variable. Esto hace que sea difícil mantener el código ya que hay que modificarlo en varios lugares.

\end{itemize}

\subsection{Problemas de rendimiento}

Al analizar el rendimiento del código, se encontraron dos áreas de potenciales mejoras:

\begin{itemize}
    \item \textbf{Remoción de offset:} El algoritmo de remoción de offset es muy ineficiente. Como se puede observar en la implementación \nameref{lst:codigo_remocion_offset}, para cada punto, se hace una suma de 5001 valores divididos por 5001,
    resultando en una complejidad $$O(5001n) = O(n)$$ Obviamente se trata de una complejidad lineal, pero no solo que la constante es altísima por el factor de 5001, sino que también se realizan 5001 divisiones por cada punto, que no es la operación más eficiente en un procesador.

    \begin{lstlisting}[language=Fortran, label=lst:codigo_remocion_offset, caption={buscador\_de\_gotas.f - Algoritmo ineficiente de remoción de offset}]
! n = 5000
! ww es la cantidad de puntos en la señal
! w1 y w2 son los arreglos de las señales
        do j=1,ww
            if ((j.ge.n/2+1).and.(j.le.ww-n/2)) then
                do k=-n/2,n/2
                       w1p(j)=w1p(j)+w1(j+k)/(n+1)
                       w2p(j)=w2p(j)+w2(j+k)/(n+1)            
                enddo
                w1a(j)=w1(j)-w1p(j)
                w2a(j)=w2(j)-w2p(j)
            else 
                ind(j)=0
                w1a(j)=0
                w2a(j)=0
            endif	
            write(30,*) j,w1a(j),w2a(j),ind(j)
        enddo
    \end{lstlisting}


    \item \textbf{Detección de pulsos:} Si analizamos la complejidad de la implementación de este algoritmo, podemos ver que para cada umbral (1000 umbrales), se revisa todo el array de la señales. Si dos puntos consecutivos superan el umbral en la primera señal, se revisa hasta un total de 100 puntos en la segunda señal. Esto resulta en una complejidad $$O(\#umbrales * \#puntos * 100) = O(100000 * \#puntos) = O(\#puntos)$$ Idéntico al caso anterior, se trata de una complejidad lineal, pero con una constante extremadamente alta.
\end{itemize}

\subsection{Motivación de la refactorización}

Todas las problemáticas mencionadas anteriormente, motivan a realizar una refactorización del código. Esta refactorización tiene muchos beneficios, entre los cuales:

\begin{itemize}

\item \textbf{Mantenibilidad:} Cualquier corrección de errores o implementación de mejoras requiere revisar y entender menos código, ya que la lógica se encuentra en funciones más pequeñas.

\item \textbf{Reutilizabilidad:} El código puede ser reutilizado para otros experimentos o instrumentos.

\item \textbf{Verificación:} Permitiría realizar pruebas unitarias de cada función y no de todo el programa completo.

\item \textbf{Colaboración:} Otros investigadores pueden entender y modificar el código sin
invertir tanta cantidad de tiempo y esfuerzo estudiándolo.

\end{itemize}


\section{Algoritmos y estructuras de datos utilizadas}
\lhead{}
\rhead{Algoritmos y estructuras de datos utilizadas}

Para abordar las limitaciones de performance del código existente, se utilizaron algoritmos y estructuras de datos eficientes que mantienen la misma o similar funcionalidad pero con un rendimiento superior.

\subsection{Optimización del algoritmo de remoción de offset con ventana deslizante}

El algoritmo de ventana deslizante (sliding window) es una técnica de optimización utilizada para resolver eficientemente problemas que involucran secuencias contiguas en arreglos.

En términos generales, la técnica consiste en mover una `ventana' de tamaño fijo a lo largo de la secuencia de datos, realizando operaciones sobre los elementos contenidos en cada posición de la ventana. A medida que la ventana avanza, el valor más antiguo se elimina y el nuevo valor que ingresa se incorpora al cálculo, actualizando así el resultado con un costo computacional que depende solamente de la operación realizada, y no del tamaño de la ventana.

En la figura \ref{fig:sliding_window_algorithm} se ilustra el funcionamiento para calcular la suma de todos los subarreglos de tamaño 3, mostrando cómo el algoritmo evita recalcular la suma completa en cada desplazamiento de la ventana.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sliding_window_algorithm.png}
    \caption{Ejemplo de algoritmo de ventana deslizante para calcular la suma de todos los subarreglos de tamaño 3.}
    \label{fig:sliding_window_algorithm}
\end{figure}

Una vez entendido este principio, podemos aplicar la técnica al problema específico de cálculo del promedio móvil utilizado en este trabajo para remover el offset de la señal. En este caso, se mantiene una ventana de tamaño fijo que se desliza a lo largo de la señal, calculando el promedio en cada punto. Dado que la ventana es de tamaño fijo, el promedio es simplemente la suma de la ventana (lo cual se resuelve con la técnica de sliding window) dividida por el tamaño de la ventana.

La implementación del algoritmo optimizado de \nameref{lst:codigo_promedio_movil}, utiliza esta técnica. Vemos que el arreglo de datos se recorre una sola vez. En cada posición, solo se realiza una suma para agregar el nuevo valor a la ventana y una resta para eliminar el valor más antiguo cuando la ventana está completa.

De esta manera, la suma total de la ventana se mantiene actualizada de forma incremental, requiriendo solo dos operaciones por iteración independientemente del tamaño de la ventana. El promedio se calcula dividiendo esta suma por el tamaño de la ventana y se resta al punto central para corregir el offset de este punto. Como resultado, tenemos un algoritmo de complejidad $O(n)$ con una constante muy baja, ya que hacemos una sola división por punto, y dos operaciones para mantener la suma actualizada.

\begin{lstlisting}[language=C++, label=lst:codigo_promedio_movil, caption={normalizer.cpp - Algoritmo optimizado de ventana deslizante con complejidad O(n)}]
    double sumaSensor1 = 0.0;
    double sumaSensor2 = 0.0;
    std::vector<double> normSensor1, normSensor2; // Señales normalizadas
    int mitadVentana = WINDOW_SIZE / 2; // 2500
    int tamañoVentana = 2 * mitadVentana + 1; // 5001 (considera el punto central)

    int tamañoActual = 0;

    for (int i = 0; i < data.size(); ++i) {
        // Agregamos el nuevo valor a las sumas
        sumaSensor1 += data[i].sensor1;
        sumaSensor2 += data[i].sensor2;
        ++tamañoActual; // Incrementamos el tamaño de la ventana

        // Si nos excedemos del tamaño de la ventana, restamos el valor más viejo
        if (tamañoActual > tamañoVentana) {
            sumaSensor1 -= data[i - tamañoVentana].sensor1;
            sumaSensor2 -= data[i - tamañoVentana].sensor2;
            --tamañoActual; // Decrementamos el tamaño de la ventana
        }

        // Si la ventana está completa, calculamos el promedio
        if (tamañoActual == tamañoVentana) {
            // Hacemos la división solo cuando lo necesitamos
            double promSensor1 = sumaSensor1 / tamañoVentana;
            double promSensor2 = sumaSensor2 / tamañoVentana;

            // Restamos el promedio de la señal al punto central, quitando el offset
            normSensor1.push_back(data[i - mitadVentana].sensor1 - promSensor1);
            normSensor2.push_back(data[i - mitadVentana].sensor2 - promSensor2);
        }
    }
\end{lstlisting}

\subsection{Estructura de datos MinQueue}

Para entender la estructura de datos MinQueue, primero debemos entender las colas y las colas doblemente terminadas. Una cola (queue) es una estructura de datos que sigue el principio FIFO (First In, First Out), donde los elementos se agregan por un extremo y se eliminan por el otro. Una cola doblemente terminada (deque) es una estructura de datos que permite insertar y eliminar elementos por ambos extremos. Estas estructuras de datos son muy útiles para modelar situaciones comunes, como la cola del supermercado (ejemplo clásico de una queue) o una escalera mecánica que pueda cambiar de dirección (ejemplo clásico de una deque).

La \textit{MinQueue} utiliza estas 2 estructuras internamente, para poder no solo mantener una queue de datos, sino que también permite obtener el elemento mínimo en tiempo constante ($O(1)$) en vez de buscar el mínimo en toda la ventana cada vez ($O(n)$).

\subsubsection{Operaciones}

Una queue tiene dos operaciones basicas: \texttt{push} y \texttt{pop}. La primera agrega un elemento al principio de la queue, y la segunda elimina el elemento al final de la queue.

La deque, tiene 2 operaciones extras: \texttt{push\_back} y \texttt{pop\_front}. La primera agrega un elemento al final de la deque, y la segunda elimina el elemento al principio de la deque.

Estas 4 operaciones, se pueden realizar en tiempo constante ($O(1)$).

La MinQueue utiliza una queue principal para almacenar los elementos y una deque para mantener los candidatos al mínimo. Además, agrega la operación \texttt{min} para obtener el elemento mínimo actual. La clave del algoritmo radica en mantener la deque en orden creciente, para que el primer elemento de esta siempre sea el mínimo actual.

En la figura \ref{fig:minqueue_algorithm} se puede observar un ejemplo de funcionamiento de algunas operaciones de la MinQueue. Se debe notar que la queue mantiene los elementos en orden FIFO, y la deque tiene la particularidad de que se elimina el último elemento (derecha) mientras sea mayor al elemento que se está agregando. Un ejemplo de implementación de la MinQueue se puede observar en \nameref{lst:minqueue_simple}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/minqueue_algorithm.png}
    \caption{Ejemplo de funcionamiento de algunas operaciones de la MinQueue.}
    \label{fig:minqueue_algorithm}
\end{figure}

\begin{lstlisting}[language=C++, label=lst:minqueue_simple, caption={MaxMinQueue.hpp - Implementación básica de MinQueue con complejidad O(1) amortizado}]
class MinQueue {
private:
    std::queue<int> queue;
    std::deque<int> deque;

public:
    void push(int value) {
        queue.push(value);
        
        // Mantener el deque en orden creciente
        while (!deque.empty() && deque.back() > value) {
            deque.pop_back();
        }
        deque.push_back(value);
    }
    
    int pop() {
        if (queue.empty()) {
            throw std::runtime_error("Queue is empty");
        }
        
        int value = queue.front();
        queue.pop();
        
        // Si el elemento que sacamos es el mínimo actual
        if (value == deque.front()) {
            deque.pop_front();
        }
        
        return value;
    }
    
    int min() const {
        if (deque.empty()) {
            throw std::runtime_error("Queue is empty");
        }
        return deque.front();
    }
    
    bool empty() const {
        return queue.empty();
    }
};
\end{lstlisting}

\subsubsection{Análisis de complejidad}

\begin{itemize}
    \item \textbf{push(value):} $O(1)$ amortizado. Aunque el bucle while puede eliminar múltiples elementos, cada elemento se elimina exactamente una vez durante toda su vida en la cola, por lo que la complejidad amortizada es constante.
    
    \item \textbf{pop():} $O(1)$. Solo requiere eliminar el primer elemento de ambas colas.
    
    \item \textbf{min():} $O(1)$. El mínimo siempre está en el frente del deque auxiliar.
    
    \item \textbf{empty():} $O(1)$. Verificación directa del estado de la cola principal.
\end{itemize}

\subsubsection{Extensión a MaxQueue}

Es trivial adaptar la MinQueue para obtener una MaxQueue: simplemente cambiamos la condición de comparación en el bucle while de `$>$' a `$<$', manteniendo el deque auxiliar en orden decreciente en lugar de creciente.

\subsubsection{MaxMinQueue: Manteniendo mínimo y máximo}

Para mantener simultáneamente el mínimo y el máximo, utilizamos dos deques auxiliares independientes: uno para el mínimo (orden creciente) y otro para el máximo (orden decreciente). Esta extensión mantiene la complejidad $O(1)$ amortizado para todas las operaciones, aunque requiere el doble de espacio auxiliar.

\section{Mejoras aplicadas en el algoritmo}

\subsection{Mejoras en el preprocesamiento de señales}

Se implementaron dos mejoras en el preprocesamiento de señales. La primera es una mejora en el algoritmo de rellenado de huecos, y la segunda es una optimización en el algoritmo de remoción del offset, utilizando un algoritmo de ventana deslizante explicado previamente.

\subsubsection{Mejora en el algoritmo de rellenado de huecos}

La mejora implementada consiste en utilizar los promedios de los puntos anteriores y posteriores al hueco en vez de valores puntuales para la interpolación. Esta optimización proporciona mejores resultados por las siguientes razones:

\begin{itemize}
    \item \textbf{Mayor robustez estadística:} Al utilizar promedios de ventanas amplias en lugar de valores puntuales, se reduce la sensibilidad a valores atípicos o ruido en los extremos del hueco.
    
    \item \textbf{Mejor representación del comportamiento de la señal:} Los promedios de 1000 puntos proporcionan una mejor estimación del nivel de señal en la región circundante al hueco, especialmente importante cuando los valores extremos pueden no ser representativos del comportamiento general de la señal.
    
    \item \textbf{Reducción de distorsiones:} Esta mejora minimiza las distorsiones que pueden introducirse durante el rellenado, lo cual es particularmente importante para los pasos posteriores del procesamiento, especialmente en la remoción del offset.
\end{itemize}

En la figura \ref{fig:mejora_rellenado_huecos} se puede observar que con el método anterior, a la señal se le termina quitando offset "de más", ya que la presencia de valores anómalos en los extremos del hueco afecta el cálculo posterior del promedio de las ventanas.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/mejora_rellenado_huecos.png}
    \caption{Diferencia entre la interpolación puntual y la interpolación con promedios de ventanas}
    \label{fig:mejora_rellenado_huecos}
\end{figure}

\subsubsection{Optimización del algoritmo de remoción del offset}

En el algoritmo original de remoción de offset implementado en `buscador\_de\_gotas.f', se utilizaba un enfoque ineficiente para calcular el promedio de los 5001 puntos circundantes para cada punto de la señal.

Para optimizarlo, se aplicó el algoritmo antes descrito de ventana deslizante para el cálculo del promedio, reduciendo significativamente la constante del algoritmo.

\subsection{Cambios en la detección de pulsos}
\lhead{}
\rhead{Cambios en la detección de pulsos}

Se implementó un nuevo algoritmo de detección de pulsos, que reemplaza la búsqueda con umbrales adaptativos por una búsqueda local e iterativa.

\subsubsection{Algoritmo básico}

El nuevo algoritmo de detección de pulsos se basa en una estrategia de búsqueda local e iterativa que aprovecha el conocimiento previo sobre las características físicas de las gotas. La premisa fundamental es que los pulsos tienen un tamaño máximo fijo de \texttt{DROP\_SIZE = 400} puntos, lo que permite realizar la búsqueda en ventanas de tamaño fijo de \texttt{2 $\times$ DROP\_SIZE = 800} puntos.

El algoritmo funciona de la siguiente manera:

\begin{enumerate}
    \item \textbf{Búsqueda de los puntos críticos del pulso:} En una ventana de datos, se buscan los puntos críticos del pulso, que son los puntos de mayor valor absoluto del sensor del anillo y del sensor de la placa, denominados \texttt{c1} y \texttt{c2} respectivamente. Para evitar escoger puntos anómalos, se toma mínimo (máximo para pulsos negativos) entre los puntos \texttt{i} e \texttt{i+1} del sensor del anillo y del sensor de la placa. La separación máxima entre los puntos críticos es de \texttt{nn = 100} puntos, y además esta búsqueda se hace con la restricción de que \texttt{c1 <\ DROP\_SIZE} (ya que si \texttt{c1} se acerca al borde derecho, el pulso podría verse truncado).
    
    \item \textbf{Análisis del pulso detectado:} Si se encuentran \texttt{c1} y \texttt{c2}, se procede a encontrar los puntos de inicio del pulso, el punto medio de la señal del anillo y el punto de quiebre de la señal de la placa. Con estos datos se calculan todas las propiedades físicas de la gota (carga, velocidad, diámetro, etc.).
    
    \item \textbf{Aplicación de filtros de calidad:} Se aplican los filtros de calidad, se computa la evaluación y se determina si el pulso es válido.
    
    \item \textbf{Manejo de resultados:} 
    \begin{itemize}
        \item Si no se encuentran \texttt{c1} y \texttt{c2}, se descartan \texttt{DROP\_SIZE} puntos como candidatos y se avanza la ventana.
        \item Si el pulso no es válido, se descartan solo los puntos \texttt{c1} y \texttt{c2} (y sus siguientes) encontrados y se reinicia la búsqueda en la misma ventana, lo que ayuda a manejar puntos anómalos en la señal.
        \item Si el pulso es válido, se marcan todos los puntos del pulso como utilizados y se reinicia la búsqueda en la misma ventana, permitiendo detectar pulsos múltiples cuando estos son muy cortos.
    \end{itemize}
\end{enumerate}

Al avanzar la ventana, se permite que hasta los primeros \texttt{nn = 100} puntos de la ventana se encuentren marcados como utilizados, ya que pueden contener el inicio de un pulso que se va a procesar en la siguiente ventana. Esto hace que la ventana siempre avance como mínimo 300 puntos (ya que la búsqueda agota todos los puntos críticos de la ventana, y en el último paso no se encontrará ninguno, marcando así \texttt{DROP\_SIZE} puntos como utilizados).

Esta estrategia iterativa tiene una gran ventaja en cuanto a la anterior, ya que una vez encontrados los puntos relevantes del pulso, analiza si la gota es válida o no, evitando descartar puntos que no pertenecen a ningún pulso.

\subsubsection{Análisis detallado del algoritmo}

\subsubsubsection{Preprocesamiento de señales}

El algoritmo empieza preprocesando los datos de los sensores para calcular un \texttt{value} para cada punto. Para ambos sensores, el \texttt{value} se calcula de la siguiente manera:

$$
\texttt{value[i]} = \min(\texttt{sensor[i]}, \texttt{sensor[i+1]}) \quad \text{para pulsos positivos}
$$
$$
\texttt{value[i]} = \max(\texttt{sensor[i]}, \texttt{sensor[i+1]}) \quad \text{para pulsos negativos}
$$

Si el punto \texttt{i} o \texttt{i+1} está marcado como descartado, el \texttt{value} se establece en 0, excluyendo efectivamente ese punto de la búsqueda.

Este paso se puede implementar con una sola iteración sobre la señal, resultando en una complejidad de $O(n)$, donde $n$ es el tamaño de la ventana.

\subsubsubsection{Búsqueda iterativa con MaxMinQueue}

Para encontrar los puntos \texttt{c1} y \texttt{c2}, primero buscaremos el `mejor' \texttt{c2} para cada posición `\texttt{i}' del sensor del anillo.

Para esto, utilizamos una MaxMinQueue para mantener el mínimo y máximo de los últimos \texttt{nn}\ \texttt{values} del sensor de la placa, mientras se va recorriendo la señal del anillo desde el final hacia el inicio. De esta manera, para cada `\texttt{i}' sabemos cuál es el `mejor' \texttt{c2} y su posición para el `\texttt{c1 = i}'.

Al mismo tiempo, podemos mantener el mejor de estos pares de puntos `\texttt{c1}' y `\texttt{c2}', comparando su valor absoluto con el mejor valor absoluto encontrado hasta el momento. De esta manera, al final de la búsqueda, tenemos el mejor par de puntos `\texttt{c1}' y `\texttt{c2}' encontrado para esta ventana.

Al final, se debe comparar tanto \texttt{c1} como \texttt{c2} con el umbral mínimo en valor absoluto (\texttt{MINIMUM\_THRESHOLD = 0.02}) y verificar que sean de la misma polaridad. En caso de que no cumplan alguna de las dos condiciones, no habremos encontrado ningún \texttt{c1} y \texttt{c2} válidos para esta ventana, y marcaremos los primeros \texttt{DROP\_SIZE} puntos como utilizados.

Este paso se puede implementar también con una sola iteración sobre la señal, así que a priori la complejidad es $O(n)$. Las operaciones de MaxMinQueue tienen complejidad constante amortizada, así que no afectan a la complejidad total.

En caso de que debamos marcar toda la ventana como utilizada, el coste es también de complejidad lineal (solo hay que iterar sobre la ventana una vez, marcando cada punto como utilizado).

\subsubsubsection{Evaluación del mejor par de puntos críticos}

Una vez obtenido el mejor par de puntos críticos, podemos calcular todos los puntos relevantes del pulso, y aplicar los filtros de calidad para decidir si el pulso es válido o no. Si el pulso no es válido, se descartan los puntos críticos encontrados (se marcan como utilizados), y se reinicia la búsqueda en la misma ventana.

Si el pulso es válido, se calculan todas las propiedades físicas de la gota, y se escribe en el archivo de resultados. Luego, se marcan como utilizados todos los puntos del pulso y se reinicia la búsqueda en la misma ventana también (esto es importante para poder detectar pulsos múltiples cuando estos son muy cortos).

Debido a la escasez de pulsos válidos con respecto a la totalidad de los datos, el tiempo de ejecución de este paso es despreciable.

\subsubsubsection{Avance de la ventana}

La ventana no se avanza hasta que todos los puntos hayan sido utilizados. Cuando esto ocurra, la ventana se avanzará como mínimo 300 puntos (dado que los primeros 100 puntos permitimos que estén marcados como utilizados). Esto nos da como resultado una cantidad total de ventanas $O(n / 300) = O(n)$, donde $n$ es la cantidad total de puntos en la señal.

\subsubsubsection{Análisis de complejidad}

Habiendo analizado las complejidades de cada componente del algoritmo por separado, podemos determinar la complejidad total del sistema.

El algoritmo procesa $O(n/300)$ ventanas, donde $n$ es el número total de puntos en la señal. En cada ventana de tamaño $V = 800$ puntos, se realizarán búsquedas hasta agotar los candidatos a puntos críticos.

Durante cada búsqueda, se marcan como mínimo 4 puntos como utilizados. Esto establece una cota superior de $300/4 = 75$ búsquedas por ventana. En la práctica, cuando se detectan pulsos válidos (que obviamente abarcan más de 4 puntos), se reduce significativamente el número real de búsquedas por ventana.

Cada operación de búsqueda individual tiene complejidad $O(V)$, donde $V$ es el tamaño de la ventana (800 puntos). Por tanto, la complejidad total del algoritmo es:

$$O\left(\frac{n}{300} \times 75 \times 800 \right) = O(200n) = O(n)$$

Esta reducción de constante representa una mejora sustancial respecto al algoritmo original, que habíamos analizado anteriormente y cuya constante era de $100000$.

\section{Sistema de Automatización}
\lhead{}
\rhead{Sistema de Automatización}

Para cumplir con el objetivo de desarrollar un sistema de automatización que permita ejecutar la cadena completa de análisis con un único comando, se implementó un script en Python (\texttt{run.py}) que orquesta todo el proceso de análisis.

\subsection{Funcionalidades Principales}

El sistema automatiza las siguientes tareas:

\begin{itemize}
    \item \textbf{Compilación automática:} Compila todos los programas C++ necesarios usando el sistema \texttt{make}
    
    \item \textbf{Cadena de procesamiento:} Ejecuta secuencialmente \texttt{drop\_finder}, \texttt{drop\_sorter}, \texttt{drop\_chart} y \texttt{carga\_velocidad} para cada archivo de datos
    
    \item \textbf{Procesamiento paralelo:} Permite procesar múltiples archivos simultáneamente utilizando múltiples procesos
    
    \item \textbf{Control de ejecución:} Incluye opciones para reiniciar desde pasos específicos y manejo de errores
\end{itemize}

\subsection{Interfaz de Usuario}

El sistema se ejecuta desde la línea de comandos con la siguiente sintaxis:

\begin{verbatim}
python run.py archivo1.lvm archivo2.lvm [opciones]
\end{verbatim}

Las opciones principales incluyen:
\begin{itemize}
    \item \texttt{--clean}: Limpiar compilaciones previas
    \item \texttt{--from-step N}: Reiniciar desde el paso N
    \item \texttt{--processes N}: Usar N procesos paralelos
\end{itemize}

Este sistema elimina la necesidad de ejecutar manualmente cada programa por separado y permite el procesamiento eficiente de grandes volúmenes de datos, simplificando el proceso de análisis.

\section{Resultados}
\lhead{}
\rhead{Resultados}

En esta sección se presentan los resultados obtenidos con el nuevo sistema de procesamiento, evaluando el cumplimiento de los objetivos planteados y comparando el rendimiento con el sistema original.

\subsection{Cumplimiento de Objetivos}

Todos los objetivos específicos planteados fueron cumplidos exitosamente:

\subsubsection{Arquitectura Modular}

Se logró diseñar una arquitectura completamente modular que separa las etapas del procesamiento en componentes independientes y reutilizables:

\begin{itemize}
    \item \textbf{Preprocesamiento:} Módulo independiente para rellenado de huecos y remoción de offset
    \item \textbf{Detección de pulsos:} Algoritmo optimizado con estructura de datos eficiente
    \item \textbf{Análisis y filtrado:} Sistema de evaluación y filtros de calidad
    \item \textbf{Automatización:} Script de Python que orquesta todo el proceso
\end{itemize}

\subsubsection{Optimización de Rendimiento y Memoria}

El nuevo sistema superó significativamente los objetivos de rendimiento planteados:

\begin{itemize}
    \item \textbf{Tiempo de procesamiento:} Se redujo de 6 horas a 5 minutos para una tormenta de 5 horas de duración (aproximadamente 100 millones de datos), representando una mejora del 7200\%.
    \item \textbf{Uso de memoria:} El sistema utiliza menos de 2 GB de memoria RAM, cumpliendo con el límite establecido.
    \item \textbf{Precisión:} Se detectó un 5\% más de gotas que el sistema original, manteniendo la misma calidad de detección.
\end{itemize}

\subsubsection{Sistema de Automatización}

Se desarrolló un sistema de automatización completo que permite ejecutar toda la cadena de análisis con un único comando:

\begin{verbatim}
python run.py archivo1.lvm archivo2.lvm
\end{verbatim}

El sistema incluye procesamiento paralelo, manejo de errores y opciones de configuración avanzadas.

\subsubsection{Documentación}

Se documentó completamente el sistema mediante:
\begin{itemize}
    \item Manual de usuario detallado
    \item Comentarios extensivos en el código
    \item Documentación de algoritmos y estructuras de datos
    \item Guías de instalación y configuración
\end{itemize}

\subsection{Mejoras en la Calidad de Detección}

Además de la mejora cuantitativa en la cantidad de gotas detectadas, se observó una mejora cualitativa en la calidad de las detecciones. Como se puede ver en la figura \ref{fig:calidad_detecciones}, si observamos la proporción acumulada de la evaluación de las gotas obtenidas, vemos que el nuevo sistema obtiene, aunque por un margen pequeño, mejor calidad de gotas que el sistema original. Esto nos dice que las gotas extras detectadas tienen en general un buen puntaje de evaluación.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/calidad_detecciones.png}
    \caption{Proporción acumulada de evaluación de ambos algoritmos}
    \label{fig:calidad_detecciones}
\end{figure}

\section{Conclusiones}
\lhead{}
\rhead{Conclusiones}

En este trabajo se desarrolló un sistema modular y automatizado para el análisis de datos de gotas cargadas eléctricamente que superó significativamente los objetivos planteados.

\subsection{Logros Principales}

Se consiguió una mejora del 7200\% en el tiempo de procesamiento, reduciendo el análisis de una tormenta de 5 horas de 6 horas a solo 5 minutos. Esta mejora se logró mediante la implementación de algoritmos optimizados como ventanas deslizantes y estructuras de datos eficientes.

El sistema de automatización desarrollado elimina completamente la intervención manual, permitiendo procesar grandes volúmenes de datos con un único comando.

Además de las mejoras cuantitativas, el sistema logró una mejora cualitativa del 5\% en la detección de gotas, con una distribución de calidad más favorable que el sistema original. Esto se debe principalmente a las mejoras en el preprocesamiento de señales y la detección de pulsos.

\subsection{Trabajo Futuro}

Las mejoras implementadas abren varias líneas de investigación prometedoras:

\begin{itemize}
    \item \textbf{Mejora en la adquisición de datos:} El desarrollo de hardware que elimine los huecos de 50ms cada segundo permitiría obtener datos más completos y precisos.
    
    \item \textbf{Procesamiento en tiempo real:} Dado que 5 horas de datos se procesan en 5 minutos, es técnicamente viable implementar un sistema de procesamiento en tiempo real que solo almacene las gotas detectadas, eliminando la necesidad de apagar y encender el instrumento.
    
    \item \textbf{Métodos de detección avanzados:} La exploración de técnicas de machine learning como Support Vector Machines (SVM) o redes neuronales podría mejorar aún más la precisión de detección, especialmente en condiciones meteorológicas complejas.
\end{itemize}

\subsection{Reflexión Final}

Este trabajo demuestra cómo la optimización algorítmica y el diseño de software pueden transformar completamente la capacidad de análisis científico. La reducción de 6 horas a 5 minutos no es solo una mejora cuantitativa, sino que representa un cambio cualitativo en las posibilidades de investigación.

El sistema desarrollado no solo cumple con todos los objetivos planteados, sino que los supera significativamente, proporcionando una herramienta robusta y eficiente para el análisis de datos meteorológicos. La arquitectura modular y la documentación completa aseguran que el sistema sea sostenible y extensible para futuras investigaciones.

\printbibliography

\end{document}
